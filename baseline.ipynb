{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\geral\\miniconda3\\envs\\AV\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from Dataset.CustomDataset import AgeGroupAndAgeDataset, StandardDataset, AgeGroupAndAgeDatasetKL\n",
    "from Dataset.CustomDataLoaders import CustomDataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from Utils import AAR, CSVUtils, AgeConversion\n",
    "from Utils.Validator import Validator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricamento del dataframe\n",
    "df = CSVUtils.get_df_from_csv(\"./training_caip_contest.csv\", \"./training_caip_contest/\")\n",
    "\n",
    "#Suddivisione del dataframe in 3 age groups\n",
    "_, d = CSVUtils.get_df_with_age_subdivision(df, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting tra Train e Validation set\n",
    "df_train, df_val = train_test_split(df, test_size=0.25, random_state=42)\n",
    "#Aggiornamento degli indici per pandas\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "transform_func = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(0.1, 0.1, 0.1, 0.1),\n",
    "    transforms.RandomGrayscale(),\n",
    "])\n",
    "\n",
    "transform_func_val = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "#Implementazione di un Dataset utilizzando \"CustomDataset\" per l'architettura con Film\n",
    "cd_train = AgeGroupAndAgeDatasetKL(df_train, path_col=\"path\", label_col=\"age\", label_function=\"CAE\", \n",
    "                                 label_map=d, label_map_n_classes=3, transform_func=transform_func)\n",
    "\n",
    "#Implementazione di un Dataset che adatta le label all'utilizzo che vogliamo farne (in questo caso CAE = Cathegorical)\n",
    "cd_val = StandardDataset(df_val, path_col=\"path\", label_col=\"age\", label_function=\"CAE\", transform_func=transform_func_val)\n",
    "\n",
    "#Dato che lo split potrebbe non prendere sample di determinate classi facciamo il set del numero di classi\n",
    "cd_train.set_n_classes(101)\n",
    "\n",
    "#Loader che, conoscendo la grandezza del dataset, farà lo shuffle dei campioni e crea i batch\n",
    "dm_train, dm_val = CustomDataLoader(cd_train), CustomDataLoader(cd_val)\n",
    "#Utilizziamo il dataloader che crea batch bilanciati implementato in CustomDataLoaders\n",
    "# In generale non usare questo ma l'unbalanced dato che non è usato in nessun doc\n",
    "dl_train = dm_train.get_unbalanced_dataloader(batch_size=128 ,shuffle=True, num_workers=6, prefetch_factor=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Initialize the ResNet18 model with pre-trained parameters on ImageNet\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "# Change the dimension of the fully connected layer to K\n",
    "K = 101\n",
    "model.fc = nn.Linear(model.fc.in_features, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validator che si occuperà di tenere in considerazione le metriche da massimizzare per il contest\n",
    "validator = Validator(cd_val, AgeConversion.ArgMaxAge, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the transformation that resizes the image and flips it horizontally with a probability of 0.5\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the KL divergence loss\n",
    "def kl_divergence_loss(pred, target):\n",
    "    # Calculate the KL divergence between the label distribution and the predicted age distribution\n",
    "    loss = nn.KLDivLoss(reduction='batchmean')(pred.log(), target)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Define the L1 loss\n",
    "def l1_loss(pred, target):\n",
    "    # Calculate the L1 loss between the predicted age and the ground truth label\n",
    "    loss = nn.L1Loss()(pred, target)\n",
    "    return loss\n",
    "\n",
    "# Combine the KL divergence loss and L1 loss\n",
    "def combined_loss(pred, target):\n",
    "    return kl_divergence_loss(pred, target) + l1_loss(pred, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 29/3370 [01:14<2:23:06,  2.57s/ batch, loss=2.585818169013351] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     15\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(dl_train, unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m batch\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m tepoch:\n\u001b[1;32m---> 16\u001b[0m     \u001b[39mfor\u001b[39;00m \u001b[39minput\u001b[39m, target \u001b[39min\u001b[39;00m tepoch:\n\u001b[0;32m     17\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m         target \u001b[39m=\u001b[39m target[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\geral\\miniconda3\\envs\\AV\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\geral\\miniconda3\\envs\\AV\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\geral\\miniconda3\\envs\\AV\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\geral\\miniconda3\\envs\\AV\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\geral\\miniconda3\\envs\\AV\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\geral\\Desktop\\Progetti\\AgeEstimationProject\\Dataset\\CustomDataset.py:62\u001b[0m, in \u001b[0;36mStandardDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mndarray]:\n\u001b[1;32m---> 62\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_image(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_img_paths[idx]), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_label(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_img_labels[idx])\n",
      "File \u001b[1;32mc:\\Users\\geral\\Desktop\\Progetti\\AgeEstimationProject\\Dataset\\CustomDataset.py:53\u001b[0m, in \u001b[0;36mStandardDataset._get_image\u001b[1;34m(self, image_path)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_image\u001b[39m(\u001b[39mself\u001b[39m, image_path):\n\u001b[1;32m---> 53\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform_func(Image\u001b[39m.\u001b[39;49mopen(image_path))\n",
      "File \u001b[1;32mc:\\Users\\geral\\miniconda3\\envs\\AV\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\geral\\miniconda3\\envs\\AV\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\geral\\miniconda3\\envs\\AV\\lib\\site-packages\\torchvision\\transforms\\transforms.py:1251\u001b[0m, in \u001b[0;36mColorJitter.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m   1249\u001b[0m     img \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39madjust_brightness(img, brightness_factor)\n\u001b[0;32m   1250\u001b[0m \u001b[39melif\u001b[39;00m fn_id \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m contrast_factor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1251\u001b[0m     img \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49madjust_contrast(img, contrast_factor)\n\u001b[0;32m   1252\u001b[0m \u001b[39melif\u001b[39;00m fn_id \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m saturation_factor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1253\u001b[0m     img \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39madjust_saturation(img, saturation_factor)\n",
      "File \u001b[1;32mc:\\Users\\geral\\miniconda3\\envs\\AV\\lib\\site-packages\\torchvision\\transforms\\functional.py:890\u001b[0m, in \u001b[0;36madjust_contrast\u001b[1;34m(img, contrast_factor)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m    888\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39madjust_contrast(img, contrast_factor)\n\u001b[1;32m--> 890\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49madjust_contrast(img, contrast_factor)\n",
      "File \u001b[1;32mc:\\Users\\geral\\miniconda3\\envs\\AV\\lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:194\u001b[0m, in \u001b[0;36madjust_contrast\u001b[1;34m(img, contrast_factor)\u001b[0m\n\u001b[0;32m    192\u001b[0m dtype \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mdtype \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mis_floating_point(img) \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mfloat32\n\u001b[0;32m    193\u001b[0m \u001b[39mif\u001b[39;00m c \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m--> 194\u001b[0m     mean \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmean(rgb_to_grayscale(img)\u001b[39m.\u001b[39;49mto(dtype), dim\u001b[39m=\u001b[39;49m(\u001b[39m-\u001b[39;49m\u001b[39m3\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), keepdim\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    195\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     mean \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(img\u001b[39m.\u001b[39mto(dtype), dim\u001b[39m=\u001b[39m(\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "best_val_aar = -1\n",
    "# Define the optimization algorithm\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 40, 60], gamma=0.1)\n",
    "\n",
    "model.to(\"cuda\")\n",
    "# Train the model for a total of 75 epochs\n",
    "for epoch in range(75):\n",
    "    # Train the model on the training data\n",
    "    model.train()\n",
    "    with tqdm(dl_train, unit=\" batch\") as tepoch:\n",
    "        for input, target in tepoch:\n",
    "            input = input.to(\"cuda\")\n",
    "            target = target[-1].to(\"cuda\")\n",
    "            # Clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = F.softmax(model(input), dim=-1)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = combined_loss(output, target)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            tepoch.set_postfix(loss=loss.detach().cpu().numpy()) \n",
    "        \n",
    "    # Decrease the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    def forward_function(x):\n",
    "        return F.softmax(model(x), dim=-1)\n",
    "\n",
    "    val_aar, val_aar_old = validator.validate(forward_function)\n",
    "    print(val_aar, val_aar_old)\n",
    "\n",
    "    if val_aar > best_val_aar:\n",
    "        best_val_aar = val_aar\n",
    "        torch.save(model.state_dict(), \"./model_age_baseline.pt\")\n",
    "        print(\"Model saved\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eaf4869da21e1db72c76fb1a3d42f0021933429756275c00fff16629dcda963c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
